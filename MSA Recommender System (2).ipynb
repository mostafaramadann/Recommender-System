{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.linalg import  Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameFilter(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def Filter(df):\n",
    "        output = df.spark.sql('select sid,cid,GPA,Grade from '+df.name+' where GPA > 0')\n",
    "        n_courses_df = output \\\n",
    "        .groupby('sid') \\\n",
    "        .count()\n",
    "        \n",
    "        output = output \\\n",
    "        .join(n_courses_df, on='sid', how='inner') \\\n",
    "        .filter('count > 6') \\\n",
    "        .drop('count')\n",
    "        \n",
    "        n_students_df = output \\\n",
    "        .groupby('cid') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('cid', 'cid')\n",
    "        output= output \\\n",
    "        .withColumnRenamed('cid', 'cid') \\\n",
    "        .join(n_students_df, on='cid', how='inner') \\\n",
    "        .filter('count > 20') \\\n",
    "        .drop('count')\n",
    "\n",
    "        output = output \\\n",
    "        .groupby('sid', 'cid','GPA') \\\n",
    "        .avg('Grade') \\\n",
    "        .withColumnRenamed('avg(Grade)', 'Grade')\n",
    "        output = DataFrame(dataframe=output,name='output',spark=df.spark)\n",
    "        output.grade_From_Double_To_Int()\n",
    "        \n",
    "        return output.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Class Insights######################################\n",
    "class Insights(object):\n",
    "    \n",
    "    def __init__(self,name,spark):\n",
    "        self.name=name\n",
    "        self.spark=spark\n",
    "        \n",
    "    def showInsight(self,sqlstatement,no_records=10):\n",
    "        if sqlstatement!='':\n",
    "            self.spark.sql(sqlstatement).limit(no_records).show()\n",
    "\n",
    "    def showTopStudents_Fac(self,faculty='',no_records=10):###########This function is to be implemented after clustering##\n",
    "        print(\"Under Construction\")\n",
    "        \n",
    "    def showTopStudents(self,no_records=10):###########This function is to be implemented after clustering####\n",
    "        self.spark.sql(\"select * from \"+self.name+\" order by GPA desc\").limit(no_records).show()\n",
    "        \n",
    " #   def showTopTakenCourses(self,faculty='',no_records=10):###########This function is to be implemented after clustering\n",
    " #       print(\"Under Construction\")\n",
    "    \n",
    " #   def showSemesterInsight(self,semester=0):###################\n",
    " #       print(\"UnderConstruction\")\n",
    "        \n",
    "#    def Visualize(self,dataframe):#####################\n",
    "#        print(\"UnderConstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################class DataFrame####################################\n",
    "class DataFrame(object):\n",
    "    \n",
    "    valid_grades = ['A','A-','B+','B','B-','C+','C','C-','D+','D','D-','F']#,'F1','F2','F3','W']\n",
    "    valid_grades_int = list(range(0,len(valid_grades)))\n",
    "    grades_dict = dict(zip(valid_grades, valid_grades_int))\n",
    "    \n",
    "    def __init__(self,dataframe=None,name='',spark=None):\n",
    "        if dataframe!=None and name!='' and spark!=None:\n",
    "            self.dataframe=dataframe\n",
    "            self.name=name\n",
    "            self.spark=spark\n",
    "            #self.insights=Insights(self.name,self.spark)\n",
    "            self.updateView()\n",
    "        else:\n",
    "            self.dataframe=None\n",
    "            self.name=None\n",
    "            self.spark=None\n",
    "            #self.insights=None\n",
    "            self.clusterer=None\n",
    "        \n",
    "    def openFile(self,path='C:',Type='csv',name='',dropna=False,spark=None):\n",
    "        if self.spark == None and spark!=None:\n",
    "            self.spark = spark\n",
    "            filetypes = {\n",
    "            'csv':self.spark.read.csv(path,header=True,inferSchema=True),\n",
    "            'json':self.spark.read.json(path)}\n",
    "            self.dataframe=filetypes[Type]\n",
    "            self.name=name\n",
    "            if dropna:\n",
    "                self.dropNulls()\n",
    "            self.updateView()\n",
    "            #self.insights=Insights(self.name,self.spark)\n",
    "            self.normalize_grades()\n",
    "           # self.clusterer=Clusterer(self)\n",
    "            self.updateView()\n",
    "   # def cluster(self):\n",
    "   #     self.clusterer.Fac_cluster()      \n",
    "    def dropNulls(self):\n",
    "        self.dataframe=self.dataframe.na.drop(how=\"all\")\n",
    "        self.updateView()\n",
    "        \n",
    "    def normalize_grades(self):#### Selects Valid Grades### and sets the data frame to it\n",
    "        orstring=\"Grade = \"\n",
    "        for i in range(0,len(self.valid_grades)):\n",
    "            nstring=\" or \"+\"Grade = \"\n",
    "            orstring +=\"'\"+self.valid_grades[i]+\"'\"\n",
    "            if i+1!=len(self.valid_grades):\n",
    "                orstring+=nstring\n",
    "        self.dataframe=self.spark.sql(\"select * from \"+self.name+\" where \"+orstring)\n",
    "        self.updateView()\n",
    "        \n",
    "    def grade_From_String_to_int(self):###### gives each grade a score############\n",
    "        udfstring_to_int=f.udf(DataFrame.string_to_int,IntegerType())\n",
    "        self.dataframe = self.dataframe.withColumn(\"Grade\",udfstring_to_int(\"Grade\"))\n",
    "        self.updateView()\n",
    "        \n",
    "    @staticmethod\n",
    "    def string_to_int(x):#######################Not to be played with######################################\n",
    "        if DataFrame.valid_grades_int[0]==0:\n",
    "            DataFrame.valid_grades_int.reverse()\n",
    "        DataFrame.grades_dict = dict(zip(DataFrame.valid_grades,DataFrame.valid_grades_int))\n",
    "        return DataFrame.grades_dict[x]\n",
    "    \n",
    "    def grade_From_Double_To_Int(self):########################## use after vector Assembler\n",
    "        udfdouble_to_int=f.udf(DataFrame.double_to_int,IntegerType())\n",
    "        self.dataframe = self.dataframe.withColumn(\"Grade\",udfdouble_to_int(\"Grade\"))\n",
    "        self.updateView()\n",
    "        \n",
    "    @staticmethod\n",
    "    def double_to_int(x):#calls double to _int best use after vector assembler\n",
    "        if DataFrame.valid_grades_int[0]==0:\n",
    "            DataFrame.valid_grades_int.reverse()\n",
    "        DataFrame.grades_dict = dict(zip(DataFrame.valid_grades, DataFrame.valid_grades_int))\n",
    "        return int(round(x))\n",
    "    \n",
    "    def grades_From_int_to_String(self):##################### grades_from_int_to_string###################\n",
    "        udfint_to_str = f.udf(DataFrame.int_to_str, StringType())\n",
    "        self.dataframe=self.dataframe.withColumn(\"Grade\",udfint_to_str(\"Grade\"))\n",
    "        #self.renameColumn(\"predicted Grade\",\"Grade\")\n",
    "        self.dataframe=self.dataframe.select('sid','cid','GPA','Grade')\n",
    "        self.updateView()\n",
    "        \n",
    "    @staticmethod\n",
    "    def int_to_str(x):###################################not to be played with#########################3\n",
    "        if DataFrame.valid_grades_int[0]==0:\n",
    "            DataFrame.valid_grades_int.reverse()\n",
    "        DataFrame.grades_dict = dict(zip(DataFrame.valid_grades_int,DataFrame.valid_grades))\n",
    "        return DataFrame.grades_dict[x]\n",
    "        \n",
    "    def renameColumn(self,cname,newName): ### Column Name(Key) and its equivelent(Value):\n",
    "            self.dataframe=self.dataframe.withColumnRenamed(cname,newName)\n",
    "            self.updateView()\n",
    "            \n",
    "    def updateView(self):\n",
    "        self.dataframe.createOrReplaceTempView(self.name)\n",
    "        \n",
    "    def createView(self,name,df):\n",
    "        df.createOrReplaceTempView(name)\n",
    "        \n",
    "    def show(self,limit=10):\n",
    "        self.dataframe.limit(limit).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradePredict(object):\n",
    "        \n",
    "    def Predict(self,courses=list(),sid=None,GPA=None,spark=None,prediction='ALS'):\n",
    "        \n",
    "        if len(courses)>0 and sid!=None and GPA!=None and GPA!=0 and spark!=None:\n",
    "            records=list()\n",
    "            for course in courses:\n",
    "                records.append((int(sid),int(course),float(GPA)))\n",
    "            rdd = spark.sparkContext.parallelize(records)\n",
    "            records = rdd.map(lambda x: Row(sid=int(x[0]), cid = int(x[1]),GPA=float(x[2])))\n",
    "            records = spark.createDataFrame(records)\n",
    "            #records.show()\n",
    "            if prediction =='RF':\n",
    "                predc = self.RandomForestPredict(records,spark)\n",
    "            elif prediction =='ALS':\n",
    "                predc = self.ALSPredict(records,spark)\n",
    "        if predc!=None:\n",
    "            return predc\n",
    "        else:\n",
    "            return None\n",
    "    def ALSPredict(self,df,spark):\n",
    "        \"\"\" must get the Grade columns in integer Representation\n",
    "        this function takes a data frame that contains the sid,cid and predicts the Grade\"\"\" \n",
    "        predc = PredictionModels.ALSmodel.transform(df)\n",
    "        predc = predc.withColumnRenamed('prediction','Grade')\n",
    "        predc = DataFrame(dataframe=predc,name='predictions',spark=spark)#####Reduce time by not creating this obj\n",
    "        predc.grade_From_Double_To_Int()\n",
    "        predc.grades_From_int_to_String()\n",
    "        predc.renameColumn('Grade','Predicted Grade')\n",
    "        #predc.show()\n",
    "        predc = predc.dataframe\n",
    "        count = predc.count()\n",
    "        predc = predc.collect()\n",
    "        predcRows = list()\n",
    "        for i in range(0,count):\n",
    "            predcRows.append(GradePredict.Row_Tuple(predc[i]))\n",
    "        return predcRows\n",
    "    def RandomForestPredict(self,df,spark):\n",
    "        \n",
    "        \"\"\" must get the Grade columns in integer Representation\n",
    "        this function takes a data frame that contains the sid,cid,GPA and predicts the Grade\"\"\"\n",
    "        ##########################Efred en Dah Course Gded(problem to be thought of) ############################\n",
    "        assembler = VectorAssembler(inputCols=['cid','GPA'],outputCol='features')\n",
    "        output = assembler.transform(df)\n",
    "        predc = PredictionModels.RFmodel.transform(output)\n",
    "        predc = predc.withColumnRenamed('prediction','Grade')\n",
    "        predc = DataFrame(dataframe=predc,name='predictions',spark=spark)#####Reduce time by not creating this obj\n",
    "        predc.grades_From_int_to_String()\n",
    "        predc.renameColumn('Grade','Predicted Grade')\n",
    "        #predc.show()\n",
    "        predc=predc.dataframe\n",
    "        count = predc.count()\n",
    "        predc = predc.collect()\n",
    "        predcRows=list()\n",
    "        for i in range(0,count):\n",
    "            predcRows.append(GradePredict.Row_Tuple(predc[i]))\n",
    "        return predcRows\n",
    "    @staticmethod\n",
    "    def Row_Tuple(row):\n",
    "        \"\"\" Takes a row from data frame and returns it's values as a tuple\"\"\"\n",
    "        tupl=list()\n",
    "        for item in row:\n",
    "            tupl.append(item)\n",
    "        return tuple(tupl)\n",
    "    @staticmethod\n",
    "    def Cluster(df):\n",
    "        output=DataFrameFilter.Filter(df)\n",
    "        ouput = output \\\n",
    "        .groupby('sid') \\\n",
    "        .agg(f.collect_set('cid').alias('courses')) \\\n",
    "        .withColumn('n_courses', f.size('courses')) \\\n",
    "        .filter('n_courses > 15') \\\n",
    "        .select('sid', f.explode('courses').alias('cid'))\n",
    "        output = output \\\n",
    "        .withColumn('one', f.lit(1)) \\\n",
    "        .toPandas() \\\n",
    "        .pivot_table(index='cid', columns=['sid'], values='one', fill_value=0)\n",
    "        output = df.spark.createDataFrame(output.reset_index())\n",
    "        assembler = VectorAssembler(inputCols=output.drop('cid').columns, outputCol=\"features\")\n",
    "        clustering_df = assembler.transform(output).select('cid', 'features')\n",
    "        clustered = PredictionModels.Kmodel.transform(clustering_df).select(\"cid\",\"prediction\")\n",
    "        df.dataframe= df.dataframe.join(clustered,df.dataframe['cid'] == clustered['cid'], how='inner')\n",
    "        df.show()\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModels(object):   \n",
    "    @staticmethod\n",
    "    def Train(df,modelName='ALS',faculties=9):\n",
    "        ###########cluster##############################\n",
    "        PredictionModels.TrainKmodel(df,faculties)\n",
    "        GradePredict.Cluster(df)\n",
    "        if modelName == 'RF' or modelName == 'ALL':\n",
    "            PredictionModels.spark=df.spark\n",
    "            output=DataFrameFilter.Filter(df)\n",
    "            assembler=VectorAssembler(inputCols=['cid','GPA'],outputCol='features')\n",
    "            output=assembler.transform(output)\n",
    "            traind,testd = output.randomSplit([0.8,0.2])\n",
    "            maxx=-99999999\n",
    "            trees = 0\n",
    "            for i in range(50,51):#############To Be Changed Before Deployment###################\n",
    "                rfc=RandomForestClassifier(labelCol='Grade',featuresCol='features',numTrees=i)\n",
    "                PredictionModels.RFmodel = rfc.fit(traind)\n",
    "                PredictionModels.RFpreds = PredictionModels.RFmodel.transform(testd)\n",
    "               # PredictionModels.RFpreds.show()\n",
    "                PredictionModels.accuracy = PredictionModels.Accuracy(modelName)\n",
    "                if PredictionModels.accuracy > maxx:\n",
    "                    maxx = PredictionModels.accuracy\n",
    "                    trees=i\n",
    "            #print(\"Number of Trees that Increase Accuracy of classification is {0}\".format(trees))\n",
    "            #print(\"With Accuracy \"+str(self.accuracy))\n",
    "        if modelName =='ALS' or modelName == 'ALL':\n",
    "            rank = 20  # number of latent factors\n",
    "            #maxIter = 10\n",
    "            #regParam=0.01  # prevent overfitting\n",
    "            output = DataFrameFilter.Filter(df)\n",
    "            traind,testd = output.randomSplit([0.8,0.2])\n",
    "            als = ALS(userCol=\"sid\", itemCol=\"cid\", ratingCol=\"Grade\", \n",
    "              rank=rank, \n",
    "              coldStartStrategy=\"drop\",\n",
    "              seed=12)\n",
    "            PredictionModels.ALSmodel = als.fit(traind)\n",
    "            PredictionModels.ALSpreds = PredictionModels.ALSmodel.transform(testd)\n",
    "            #PredictionModels.ALSpreds.show()\n",
    "            PredictionModels.Accuracy(PredictionModels.ALSpreds)\n",
    "    @staticmethod\n",
    "    def TrainKmodel(df,faculties):          \n",
    "        output=DataFrameFilter.Filter(df)\n",
    "        ouput = output \\\n",
    "        .groupby('sid') \\\n",
    "        .agg(f.collect_set('cid').alias('courses')) \\\n",
    "        .withColumn('n_courses', f.size('courses')) \\\n",
    "        .filter('n_courses > 15') \\\n",
    "        .select('sid', f.explode('courses').alias('cid'))\n",
    "        output = output \\\n",
    "        .withColumn('one', f.lit(1)) \\\n",
    "        .toPandas() \\\n",
    "        .pivot_table(index='cid', columns=['sid'], values='one', fill_value=0)\n",
    "        output = df.spark.createDataFrame(output.reset_index())\n",
    "        assembler = VectorAssembler(inputCols=output.drop('cid').columns, outputCol=\"features\")\n",
    "        clustering_df = assembler.transform(output).select('cid', 'features')\n",
    "        clustering_df.show()\n",
    "        kmeans = KMeans(featuresCol='features').setK(faculties).setSeed(1)\n",
    "        PredictionModels.Kmodel = kmeans.fit(clustering_df)\n",
    "    @staticmethod\n",
    "    def Accuracy(modelName='ALS'):\n",
    "        if modelName =='RF' or modelName=='ALL':\n",
    "            evaluator=MulticlassClassificationEvaluator(labelCol='Grade',predictionCol='prediction',metricName='accuracy')\n",
    "            print(PredictionModels.RFmodel.featureImportances)\n",
    "            return evaluator.evaluate(PredictionModels.RFpreds)\n",
    "        if modelName=='ALS' or modelName=='ALL':\n",
    "            evaluator=RegressionEvaluator(metricName='rmse',labelCol='Grade',predictionCol='prediction')\n",
    "            rmse = evaluator.evaluate(PredictionModels.ALSpreds)\n",
    "            print('rmse is '+str(rmse))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('MSA Recommender System').getOrCreate() \n",
    "df = DataFrame()\n",
    "df.openFile(path=\"C:\\\\Users\\\\Mostafa\\\\Desktop\\\\data.csv\",Type='csv',name='students',dropna=True,spark=spark)\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.renameColumn(df.dataframe.columns[0],'sid')\n",
    "df.renameColumn(df.dataframe.columns[1],'cid')\n",
    "df.grade_From_String_to_int()\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|cid|            features|\n",
      "+---+--------------------+\n",
      "|  1|(933,[1,10,11,13,...|\n",
      "|  6|(933,[2,4,5,7,8,1...|\n",
      "| 14|(933,[2,11,15,18,...|\n",
      "| 23|(933,[20,76,87,92...|\n",
      "| 24|(933,[3,10,110,12...|\n",
      "| 28|(933,[2,4,5,7,8,1...|\n",
      "| 34|(933,[5,14,31,48,...|\n",
      "| 35|(933,[1,10,11,13,...|\n",
      "| 36|(933,[85,87,139,1...|\n",
      "| 39|(933,[14,81,95,13...|\n",
      "| 41|(933,[0,1,10,11,1...|\n",
      "| 44|(933,[1,10,11,13,...|\n",
      "| 54|(933,[1,6,9,10,11...|\n",
      "| 55|(933,[6,9,17,19,2...|\n",
      "| 64|(933,[6,9,17,19,2...|\n",
      "| 67|(933,[55,59,62,63...|\n",
      "| 71|(933,[5,6,9,17,18...|\n",
      "| 74|(933,[1,5,6,9,10,...|\n",
      "| 76|(933,[9,17,19,29,...|\n",
      "| 78|(933,[1,6,9,10,11...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---+--------+-----+----+---+----------+\n",
      "| sid|cid|Semester|Grade| GPA|cid|prediction|\n",
      "+----+---+--------+-----+----+---+----------+\n",
      "|1195|  1|      56|   10|2.17|  1|         5|\n",
      "|1092|  1|      49|    9| 2.2|  1|         5|\n",
      "| 400|  1|      49|   10|2.47|  1|         5|\n",
      "|1275|  1|      47|    8|2.35|  1|         5|\n",
      "| 697|  1|      47|    7|1.23|  1|         5|\n",
      "| 684|  1|      47|    7|2.31|  1|         5|\n",
      "| 577|  1|      47|   10| 2.6|  1|         5|\n",
      "|1297|  1|      44|    3|1.45|  1|         5|\n",
      "|1295|  1|      44|    4|2.49|  1|         5|\n",
      "|1235|  1|      44|    6|1.94|  1|         5|\n",
      "+----+---+--------+-----+----+---+----------+\n",
      "\n",
      "(2,[0,1],[0.04509422308200707,0.9549057769179929])\n"
     ]
    }
   ],
   "source": [
    "PredictionModels.Train(df,modelName='ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(463, 243, 2.6, 'B'), (463, 135, 2.6, 'C'), (463, 16, 2.6, 'B')]\n"
     ]
    }
   ],
   "source": [
    "gp = GradePredict()\n",
    "courses=[16,135,243]\n",
    "sid=463\n",
    "gpa=2.6\n",
    "preds=gp.Predict(courses=courses,sid=sid,GPA=gpa,spark=spark)\n",
    "preds2=list()\n",
    "if len(preds)<len(courses):\n",
    "    cmps=list()\n",
    "    for pred in preds:\n",
    "        sid,cid,gpa,grade=pred\n",
    "        cmps.append(cid)\n",
    "    for course in courses:\n",
    "        if course not in cmps:\n",
    "            preds2.append(course)\n",
    "preds.extend(gp.Predict(courses=preds2,sid=sid,GPA=gpa,spark=spark,prediction='RF'))\n",
    "print(str(preds))\n",
    "####return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PredictionModels.TrainKmodel(df,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PredictionModels.Cluster(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
